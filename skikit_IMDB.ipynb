{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KeJxAKSvNfNy"
   },
   "source": [
    "# **``IMDB sentiment analysis with scikit-learn & custom algorithms``**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`0. Import modules`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd \n",
    "import random\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "VhIK8UDAgpMu"
   },
   "source": [
    "## **`1. Fetch data`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8RMZkur0Z7H4"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data()\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size= 0.1)\n",
    "\n",
    "word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "\n",
    "index2word = dict((i + 3, word) for (word, i) in word_index.items())\n",
    "\n",
    "index2word[0] = '[pad]'\n",
    "index2word[1] = '[bos]'\n",
    "index2word[2] = '[oov]'\n",
    "\n",
    "x_train = np.array([' '.join([index2word[idx] for idx in text]) for text in x_train])\n",
    "x_val = np.array([' '.join([index2word[idx] for idx in text]) for text in x_val])\n",
    "x_test = np.array([' '.join([index2word[idx] for idx in text]) for text in x_test])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`2. Create Vocabulary`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_voc(x_train, y_train, n, k, m):\n",
    "    '''\n",
    "    Creates the vocabulary\n",
    "\n",
    "    Args:\n",
    "        x_train(numpy.array): the training data\n",
    "        y_train(numpy.array): the category of every training data\n",
    "        n(int): the most frequent n words\n",
    "        m(int): the final m words that we are going to keep\n",
    "        k(int): the least frequent words\n",
    "    '''\n",
    "\n",
    "    voc_dict = dict()\n",
    "    for text in x_train:\n",
    "        tokens = set(text.split())\n",
    "        for token in tokens:\n",
    "            if token in voc_dict:\n",
    "                voc_dict[token] += 1\n",
    "            else:\n",
    "                voc_dict[token] = 1\n",
    "    \n",
    "    #Skip the most n frequent & k least words:\n",
    "    voc = sorted(voc_dict.items(), key = lambda x:x[1])\n",
    "    voc = voc[k:len(voc) - n]\n",
    "    return np.array([x[0] for x in voc[len(voc) - m:]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`3. Information Gain & Entropy`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IG(class_, feature):\n",
    "  classes = set(class_)\n",
    "\n",
    "  Hc = 0\n",
    "  for c in classes:\n",
    "    pc = list(class_).count(c)/len(class_)\n",
    "    Hc += - pc * math.log(pc, 2)\n",
    "  #print('Overall Entropy:', Hc)\n",
    "  feature_values = set(feature)\n",
    "\n",
    "  Hc_feature = 0\n",
    "  for feat in feature_values:\n",
    "    \n",
    "    #pf --> P(X=x)\n",
    "    pf = list(feature).count(feat)/len(feature)\n",
    "    indices = [i for i in range(len(feature)) if feature[i] == feat]\n",
    "    clasess_of_feat = [class_[i] for i in indices]\n",
    "    for c in classes:\n",
    "        #pcf --> P(C=c|X=x)\n",
    "        pcf = clasess_of_feat.count(c)/len(clasess_of_feat)\n",
    "        if pcf != 0:\n",
    "            # - P(X=x) * P(C=c|X=x) * log2(P(C=c|X=x))\n",
    "            temp_H = - pf * pcf * math.log(pcf, 2)\n",
    "            #sum for all values of C (class) and X (values of specific feature)\n",
    "            Hc_feature += temp_H\n",
    "  ig = Hc - Hc_feature\n",
    "  return ig    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "yDGjmz4UxlRi"
   },
   "source": [
    "## **`4. Create binary vectors`** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KESrOUVAhmRD",
    "outputId": "077cba85-f8c7-4070-a896-fe7d84e1c4c1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22500/22500 [09:05<00:00, 41.24it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25000/25000 [10:37<00:00, 39.20it/s] \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [00:51<00:00, 48.72it/s]\n"
     ]
    }
   ],
   "source": [
    "x_train_binary = list()\n",
    "x_test_binary = list()\n",
    "x_val_binary = list()\n",
    "\n",
    "vocabulary = create_voc(x_train, y_train, 50, 1000, 3000)\n",
    "\n",
    "for text in tqdm(x_train):\n",
    "  tokens = text.split()\n",
    "  binary_vector = list()\n",
    "  for vocab_token in vocabulary:\n",
    "    if vocab_token in tokens:\n",
    "      binary_vector.append(1)\n",
    "    else:\n",
    "      binary_vector.append(0)\n",
    "  x_train_binary.append(binary_vector)\n",
    "\n",
    "x_train_binary = np.array(x_train_binary)\n",
    "\n",
    "for text in tqdm(x_test):\n",
    "  tokens = text.split()\n",
    "  binary_vector = list()\n",
    "  for vocab_token in vocabulary:\n",
    "    if vocab_token in tokens:\n",
    "      binary_vector.append(1)\n",
    "    else:\n",
    "      binary_vector.append(0)\n",
    "  x_test_binary.append(binary_vector)\n",
    "\n",
    "x_test_binary = np.array(x_test_binary)\n",
    "\n",
    "for text in tqdm(x_val):\n",
    "  tokens = text.split()\n",
    "  binary_vector = list()\n",
    "  for vocab_token in vocabulary:\n",
    "    if vocab_token in tokens:\n",
    "      binary_vector.append(1)\n",
    "    else:\n",
    "      binary_vector.append(0)\n",
    "  x_val_binary.append(binary_vector)\n",
    "  \n",
    "x_val_binary = np.array(x_val_binary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **`5.0 Random Forest`ðŸŒ³ðŸŒ³ðŸŒ³ðŸŒ³ðŸŒ³ðŸŒ³**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Random_Forest:\n",
    "\n",
    "    def __init__(self, num_trees, m, max_depth):\n",
    "        '''\n",
    "        Initialize the variables\n",
    "        \n",
    "        Args:\n",
    "            num_trees(int): number of trees\n",
    "            m(int): number of features\n",
    "            max_depth(int): max_depth of every tree\n",
    "        '''\n",
    "        self.m = m\n",
    "        self.num_trees = num_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.ls_trees = []\n",
    "\n",
    "    def subsample(self, x_data, y_data):\n",
    "        '''\n",
    "        Shuffle the samples.There can be also duplicate samples\n",
    "\n",
    "        Args:\n",
    "            x_data(np.array): the training data\n",
    "            y_data(np.array): the category of every training data\n",
    "        \n",
    "        Returns:\n",
    "            x_sample(np.array)\n",
    "            y_sample(np.array)\n",
    "        '''\n",
    "        x_sample = list()\n",
    "        y_sample = list()\n",
    "        num_non_selected = [x for x in range(len(y_data))]\n",
    "        num_selected = []\n",
    "\n",
    "        for i in range(len(x_data)):\n",
    "            random_choice = random.choice(num_non_selected)\n",
    "            num_selected.append(random_choice)\n",
    "            x_sample.append(x_data[random_choice])\n",
    "            y_sample.append(y_data[random_choice])\n",
    "\n",
    "        x_sample = np.array(x_sample)\n",
    "        y_sample = np.array(y_sample)\n",
    "\n",
    "        return x_sample, y_sample\n",
    "\n",
    "    def subfeature(self, x_data):\n",
    "        '''\n",
    "        Shuffle the features.No duplicate feature\n",
    "\n",
    "        Args:\n",
    "            x_data(np.array): training examples\n",
    "\n",
    "        Returns:\n",
    "            feat_selected(list): the subset of features\n",
    "        '''\n",
    "        ls_feature = [x for x in range(len(x_data[0]))]\n",
    "        feat_selected = []\n",
    "        \n",
    "        for i in range(self.m):\n",
    "            random_f = random.choice(ls_feature)\n",
    "            ls_feature.remove(random_f)\n",
    "            feat_selected.append(random_f)\n",
    "        \n",
    "        return feat_selected\n",
    "    \n",
    "    def fit(self, x_train_b, y_train_b):\n",
    "        '''\n",
    "        Create num_trees ID3 Trees\n",
    "\n",
    "        Args:\n",
    "            x_train_b(np.array): training examples\n",
    "            y_train_b(np.array): category of every training example\n",
    "            max_depth(int): the max depth of every tree\n",
    "        '''\n",
    "        self.ls_trees.clear()\n",
    "        for i in range(self.num_trees):\n",
    "            id3 = ID3(self.max_depth)\n",
    "            random_x, random_y = self.subsample(x_train_b, y_train_b)\n",
    "            id3.tree = id3.fitting_tree(random_x, random_y, self.subfeature(random_x), 0)\n",
    "            self.ls_trees.append(id3)\n",
    "\n",
    "    def predict(self, x_test_b):\n",
    "        '''\n",
    "        Predict every category for every example in the given dataset with the help of the trained trees\n",
    "\n",
    "        Args:\n",
    "            x_test_b(np.array): training examples\n",
    "\n",
    "        Returns:\n",
    "            y_test_b(np.array): the category of every given example in the x_test_b\n",
    "        '''\n",
    "        y_test_b = []\n",
    "        sum = 0\n",
    "        for i in range(len(x_test_b)):\n",
    "            sum += 1\n",
    "            num_0 = 0\n",
    "            num_1 = 0\n",
    "            for k in range(self.num_trees):\n",
    "                id3 = self.ls_trees[k]\n",
    "                pre_category = id3.predict_sample(x_test_b[i], id3.tree)\n",
    "                if pre_category == 1:\n",
    "                    num_1 += 1 \n",
    "                else:\n",
    "                    num_0 += 1\n",
    "\n",
    "            if num_1 > num_0:\n",
    "                y_test_b.append(1) \n",
    "            else:\n",
    "                y_test_b.append(0)\n",
    "\n",
    "        y_test_b = np.array(y_test_b)\n",
    "        return y_test_b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the most optimized number of trees for Random Forest\n",
    "- ##### Every tree will have depth equal to 5\n",
    "- ##### Every tree will have 500 words to compare\n",
    "Comparing 4 different random forest classifiers in order to find the most optimized number of trees.The best_acc variable stores the highest accuracy score and the best_num stores the most optimized number of trees which we will train the final algorithm.Every model has different number of trees with the same depth and number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_trees = [x for x in range(1,5,2)]\n",
    "\n",
    "best_acc = 0.0\n",
    "best_num = 1\n",
    "for num_tree in num_of_trees:\n",
    "    rf = Random_Forest(num_trees=num_tree, max_depth= 5, m= 500)\n",
    "    rf.fit(x_train_binary, y_train)\n",
    "    acc = accuracy_score(y_val, rf.predict(x_val_binary))\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_num = num_tree\n",
    "\n",
    "print(\"The best number of trees is: \", best_num)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the most optimized number of features for Random Forest\n",
    "- ##### Every tree will have a depth equal to 3\n",
    "- ##### Every estimator will have the same number of trees\n",
    "Comparing different random forest classifiers in order to find the most optimized number of features.The best_acc variable stores the highest accuracy score and the best_m stores the most optimized number of trees which we will train the final algorithm.Every model has the same number of trees with the same depth, but different number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_m = [100, 200, 300, 500]\n",
    "\n",
    "best_acc = 0.0\n",
    "best_m = 0\n",
    "\n",
    "for num_m in num_of_m:\n",
    "    rf = Random_Forest(num_trees= 3, max_depth= 3, m= num_m)\n",
    "    rf.fit(x_train_binary, y_train)\n",
    "    acc = accuracy_score(y_val, rf.predict(x_val_binary))\n",
    "    print(\"finished for: \", num_m)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_m = num_m\n",
    "\n",
    "print(\"The best number of features is: \", best_m)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`5.1 ID3`ðŸŒ³**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ID3_Tree:\n",
    "    def __init__(self):\n",
    "        self.tag = None\n",
    "        self.feature = 'None'\n",
    "        self.child_nodes = []\n",
    "        self.decision = -1\n",
    "\n",
    "    def is_leaf(self):\n",
    "        if self.decision != -1:\n",
    "            return True \n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def _create_child(self, node):\n",
    "        self.child_nodes.append(node)\n",
    "\n",
    "class ID3:\n",
    "    def __init__(self, labels, max_depth = 10, dc=0):\n",
    "        '''\n",
    "        Initialization\n",
    "\n",
    "        Args:\n",
    "            labels(set): the labels in a set.Every label is a index of a column\n",
    "            max_depth(int): the max_depth to counter overfitting\n",
    "            dc(int): pre-decision\n",
    "        '''\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "        self.labels = labels\n",
    "        self.dc = dc \n",
    "        \n",
    "    def most_IG(self, x_train_i, y_train_i, labels):\n",
    "        '''\n",
    "        Finds the feature that is the best classifier\n",
    "\n",
    "        Args:\n",
    "            x_train_i(numpy.array): the training data \n",
    "            y_train_i(numpy.array): the category for every example in the training data\n",
    "            labels(set): every index(column) of every feature \n",
    "\n",
    "        Returns:\n",
    "            max_feature(int): the index with the most valuable feature in the set labels.It presents the specific column\n",
    "        '''\n",
    "        max_gain = -1\n",
    "        max_feature = -1\n",
    "\n",
    "        for f in labels:\n",
    "            #The training examples of a specific column (a column with a constant feature)\n",
    "            x_feature = [x_train_i[example][f] for example in range(len(x_train_i))]\n",
    "            feature_ig = IG(y_train_i, x_feature)\n",
    "            \n",
    "            if (feature_ig > max_gain):\n",
    "                max_gain = feature_ig\n",
    "                max_feature = f\n",
    "\n",
    "        return max_feature\n",
    "\n",
    "    def fit(self, x_train_b, y_train_b):\n",
    "        self.tree = self.fitting_tree(x_train_b, y_train_b, self.labels, self.dc)\n",
    "\n",
    "    def fitting_tree(self, x_train_b, y_train_b, labels, dc, depth = 0):\n",
    "        '''\n",
    "        Creates a decision tree with the given training data\n",
    "\n",
    "        Args: \n",
    "            x_train_b(numpy.array): the training data with every example\n",
    "            y_train_b(numpy.array): the category of every example in the training data\n",
    "            labels(set): every index(column) of every feature \n",
    "            dc(int): the pre-decided category.It is 0 or 1\n",
    "\n",
    "        Returns:\n",
    "            tree(ID3_Tree): the tree that will decide the category for every case\n",
    "        '''\n",
    "        uniques, counts = np.unique(y_train_b, return_counts=True)\n",
    "        #If there is no available training example\n",
    "        if len(y_train_b) == 0:\n",
    "            #Return the pre-decided category because there are no more available examples\n",
    "            node = ID3_Tree()\n",
    "            node.decision = dc \n",
    "            return node \n",
    "\n",
    "        #If there is only one category\n",
    "        if len(set(y_train_b)) == 1:\n",
    "            #Return the category that has been left\n",
    "            node = ID3_Tree()\n",
    "            node.decision = y_train_b[0]\n",
    "            return node\n",
    "\n",
    "        #If there is no feature to create new nodes\n",
    "        if len(labels) == 0:\n",
    "            #Return the category with the highest frequency\n",
    "            node = ID3_Tree()\n",
    "            node.decision = 0 if counts[0] > counts[1] else 1\n",
    "            return node\n",
    "\n",
    "        #Pruning\n",
    "\n",
    "        #Reach Max Depth\n",
    "        if depth == self.max_depth:\n",
    "            node = ID3_Tree()\n",
    "            if counts[0] > counts[1]:\n",
    "                node.decision = 0\n",
    "            elif counts[0] < counts[1]:\n",
    "                node.decision = 1\n",
    "            else:\n",
    "                node.decision = dc\n",
    "            return node\n",
    "\n",
    "        #If the positives are 95% or above\n",
    "        if float(counts[1])/float(len(y_train_b)) >= 0.95:\n",
    "            node = ID3_Tree()\n",
    "            node.decision = 1\n",
    "            return node\n",
    "            \n",
    "        #If the negatives are 95% or above\n",
    "        if float(counts[0])/float(len(y_train_b)) >= 0.95:\n",
    "            node = ID3_Tree()\n",
    "            node.decision = 0\n",
    "            return node\n",
    "\n",
    "        #Root node->feature\n",
    "        best_feat = self.most_IG(x_train_b, y_train_b, labels)\n",
    "        tree = ID3_Tree()\n",
    "        \n",
    "        #Find the most frequent category among the given training examples\n",
    "        if counts[0] > counts[1]:\n",
    "            m = 0\n",
    "        elif counts[0] < counts[1]:\n",
    "            m = 1\n",
    "        else:\n",
    "            m = dc\n",
    "\n",
    "        #Find every possible value of the given feature\n",
    "        f_set = set()\n",
    "        for eg in range(len(x_train_b)):\n",
    "            f_set.add(x_train_b[eg][best_feat])\n",
    "\n",
    "        #print('---->',labels.copy().remove(best_feat))\n",
    "        for bf_value in f_set:\n",
    "            #Examples with the best_feature == bf_value\n",
    "            x_examples = np.array([x_train_b[eg] for eg in range(len(x_train_b)) if x_train_b[eg][best_feat] == bf_value])\n",
    "            y_examples = np.array([y_train_b[eg] for eg in range(len(x_train_b)) if x_train_b[eg][best_feat] == bf_value])\n",
    "\n",
    "            #Create a subtree with only the given examples with x_train_b[row][col = best_feat] == bf_value\n",
    "            new_labels = labels.copy()\n",
    "            new_labels.remove(best_feat)\n",
    "            \n",
    "            sub_tree = self.fitting_tree(x_examples, y_examples, new_labels, m, depth+1)\n",
    "            sub_tree.tag = bf_value\n",
    "            sub_tree.feature = best_feat\n",
    "            tree._create_child(sub_tree)\n",
    "        \n",
    "        return tree\n",
    "\n",
    "    def predict_sample(self, x_sample, tree):\n",
    "        decided = False\n",
    "        sub_tree = tree\n",
    "        while not decided:\n",
    "            feature = sub_tree.child_nodes[0].feature \n",
    "            for sub in sub_tree.child_nodes:\n",
    "                if sub.tag == x_sample[feature]:\n",
    "                    sub_tree = sub\n",
    "            if sub_tree.decision != -1:\n",
    "                decided = True\n",
    "        return sub_tree.decision\n",
    "\n",
    "    def predict(self, x_train_b):\n",
    "        '''\n",
    "        Tests all the training examples with the help of the decision tree and returns for every example the predicted category in an array\n",
    "\n",
    "        Args:\n",
    "            tree(ID3_Tree): A tree that was produced for the method fit\n",
    "            x_train_b(numpy.array): the data that will be tested\n",
    "\n",
    "        Returns:\n",
    "            y_train_b(numpy.array): an array with the results/category for every given example\n",
    "        '''\n",
    "        tree = self.tree\n",
    "        y_train_b = list()\n",
    "        for i in range(len(x_train_b)):\n",
    "            y_train_b.append(self.predict_sample(x_train_b[i], tree))\n",
    "        \n",
    "        y_train_b = np.array(y_train_b)\n",
    "        return y_train_b\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the most optimized depth for ID3\n",
    "Comparing different ID3 initializations in order to find the most optimized depth for ID3 tree.The best_acc variable stores the highest accuracy score and the best_depth stores the most optimized depth which we will train the final algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_depth = [x for x in range(1, 11)]\n",
    "\n",
    "best_acc = 0.0\n",
    "best_depth = 0\n",
    "\n",
    "for depth in ls_depth:\n",
    "    id3 = ID3(labels= set([x for x in range(1, len(x_train_binary[0]))]), max_depth= depth)\n",
    "    id3.fit(x_train_binary, y_train)\n",
    "    acc = accuracy_score(y_val, id3.predict(x_val))\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_depth = depth\n",
    "\n",
    "print(\"The best number of depth is: \", best_depth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`5.2 Naive Bayes`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Naive_Bayes:\n",
    "\n",
    "    def __init__(self, alpha= 1):\n",
    "        '''\n",
    "        Initialization of the arrays\n",
    "\n",
    "        Contains:\n",
    "            x1c0(list): here are the possibilities that given the category c = 0, if x(i) = 1 then we store the possibility p(x(i) = 1 | c = 0).\n",
    "                        we can find easily the p(x(i) = 0 | c = 0) == 1.0 - p(x(i) = 1 | c = 0)\n",
    "            x1c1(list): here are the possibilities that given the category c = 1, if x(i) = 1 then we store the possibility p(x(i) = 1 | c = 1).\n",
    "                        we can find easily the p(x(i) = 0 | c = 1) == 1.0 - p(x(i) = 1 | c = 1)\n",
    "            pc0(float): the possibility of the category to be zero (c = 0 -> negative report).\n",
    "                        we can calculate the p(c = 1) == 1.0 - p(c = 0)\n",
    "            alpha(int): The Laplace estimator\n",
    "        '''\n",
    "        self.x1c0_array = list()\n",
    "        self.x1c1_array = list()\n",
    "        self.pc0 = 0.0\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def fit(self, x_train_binary, y_train):\n",
    "        '''\n",
    "        Calculate the possibilities and store them in the suitable array\n",
    "\n",
    "        Args: \n",
    "            x_train_binary(np.array): the training data \n",
    "            y_train_binary(np.array): the category for every example in the training data\n",
    "        '''\n",
    "        self.x1c0_array.clear()\n",
    "        self.x1c1_array.clear()\n",
    "        number_of_features = x_train_binary[0].shape[0]\n",
    "        number_of_examples = y_train.shape[0]\n",
    "\n",
    "        num_of_c0 = 0.0\n",
    "        for eg in y_train:\n",
    "            if eg == 0:\n",
    "                num_of_c0 += 1.0\n",
    "        self.pc0 = float(num_of_c0 / number_of_examples)\n",
    "        \n",
    "        # print(f'c0:{self.pc0} and c1:{1.0 - self.pc0}')\n",
    "        for feature in range(number_of_features):\n",
    "            #Laplace estimator\n",
    "            self.x1c0_array.append(1.0)\n",
    "            self.x1c1_array.append(1.0)\n",
    "\n",
    "            for eg in range(number_of_examples):\n",
    "                if (y_train[eg] == 0 and x_train_binary[eg][feature] == 1):\n",
    "                    self.x1c0_array[feature] += 1.0\n",
    "                elif (y_train[eg] == 1 and x_train_binary[eg][feature] == 1):\n",
    "                    self.x1c1_array[feature] += 1.0\n",
    "        \n",
    "        self.x1c0_array = [float((x + self.alpha)/ (num_of_c0 + 2*self.alpha)) for x in self.x1c0_array]\n",
    "        self.x1c1_array = [float((x + self.alpha)/ (number_of_examples - num_of_c0 + 2*self.alpha)) for x in self.x1c1_array]\n",
    "\n",
    "    def predict(self, x_train_binary):\n",
    "        '''\n",
    "        Tests all the training examples with the arrays x1c0, x1c0 and returns for every example the predicted category in an array\n",
    "\n",
    "        Args:\n",
    "            x_train_binary(numpy.array): the data that will be tested\n",
    "\n",
    "        Returns:\n",
    "            y_array(numpy.array): an array with the results/category for every given example\n",
    "        '''\n",
    "\n",
    "        x = np.vstack((self.x1c0_array, self.x1c1_array))\n",
    "        x1 = np.log(x)\n",
    "        x0 = np.log(1.0 - x)\n",
    "        x_train_ = x_train_binary.T\n",
    "        var = np.matmul(x1, x_train_) + np.matmul(x0, 1 - x_train_)\n",
    "        y_array = np.argmax(var, axis=0)\n",
    "\n",
    "        return y_array\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **```5.3 RNN (BigRu) ```**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "\n",
    "  def __init__(self, vocabulary, VOCAB_SIZE = 100000, SEQ_MAX_LENGTH = 250, epochs=3, verbose=1, batch_size=32):\n",
    "    self.vectorizer = self.create_vec(VOCAB_SIZE, SEQ_MAX_LENGTH, vocabulary)\n",
    "    self.imdb_bigru = self.get_bigru()\n",
    "    self.imdb_bigru.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(), metrics=['binary_accuracy'])\n",
    "    self.epochs = epochs\n",
    "    self.verbose = verbose\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "  def create_vec(self, VOCAB_SIZE, SEQ_MAX_LENGTH, vocabulary):\n",
    "    train_doc_length = 0\n",
    "    for doc in x_train:\n",
    "      tokens = str(doc).split()\n",
    "      train_doc_length += len(tokens)\n",
    "\n",
    "    vectorizer = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE, output_mode='int', ngrams=1, name='vector_text', output_sequence_length=SEQ_MAX_LENGTH, vocabulary= vocabulary)\n",
    "\n",
    "    return vectorizer\n",
    "\n",
    "  def get_bigru(self, num_layers=1, emb_size=64, h_size=64):\n",
    "    inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string, name='txt_input')\n",
    "    x = self.vectorizer(inputs)\n",
    "    x = tf.keras.layers.Embedding(input_dim=len(self.vectorizer.get_vocabulary()), output_dim=emb_size, name='word_embeddings', mask_zero=True)(x)\n",
    "    for n in range(num_layers):\n",
    "      if n != num_layers - 1:\n",
    "        x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=h_size, name=f'bigru_cell_{n}', return_sequences=True, dropout=0.2))(x)\n",
    "      else:\n",
    "        x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=h_size, name=f'bigru_cell_{n}', dropout=0.2))(x)\n",
    "\n",
    "    x = tf.keras.layers.Dropout(rate=0.5)(x)\n",
    "    o = tf.keras.layers.Dense(units=1, activation='sigmoid', name='lr')(x)\n",
    "    \n",
    "    return tf.keras.models.Model(inputs=inputs, outputs=o, name='simple_rnn')\n",
    "\n",
    "  \n",
    "  def fit(self, x_train_b, y_train_b):\n",
    "    self.imdb_bigru = self.get_bigru()\n",
    "    self.imdb_bigru.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(), metrics=['binary_accuracy'])\n",
    "    self.imdb_bigru.fit(x= x_train_b, y=y_train_b, epochs=self.epochs, verbose=self.verbose, batch_size=self.batch_size)\n",
    "\n",
    "  def predict(self, x_test_b):\n",
    "    return np.round(self.imdb_bigru.predict(x_test_b))\n",
    "\n",
    "  def get_history(self, x_train_b, y_train_b, validation_split= 0.2):\n",
    "    return self.imdb_bigru.fit(x= x_train_b, y= y_train_b, verbose= self.verbose, batch_size=self.batch_size, epochs=self.epochs, validation_split= validation_split)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Curves** :\n",
    "- > #### ``custom_curve``: \n",
    "     >> #### &ensp;&thinsp; `Args`:{**estimator**= classifier Object, **x_train**, **y_train**, **x_test**, **y_test**, **n_splits**, **title** = \"*< classifier name >*\", **zoom_out (default= False)**}\n",
    "     >> #### &ensp;&thinsp; `Returns`:{**data(dictionary)**= all the information of the classifier to create the tables later}\n",
    "- > #### ``compare_two_classification_algorithms``: \n",
    "     >> #### &ensp;&thinsp; `Args`:{**estimator1**= classifier Object, **estimator2**- classifier Object, **x_train**, **y_train**, **x_test**, **y_test**, **n_splits**, **title1** = \"*< classifier 1 name >*\", **title2** = \"*< classifier 2 name >*\", **zoom_out (default= False)**}\n",
    "     >> #### &ensp;&thinsp; `Returns`:{**data1(dicionary), data2(dictionary)**= all the information about the 1st & 2nd classifier to create the tables later}\n",
    "- > #### ``loss_plot``:\n",
    "     >> #### &ensp;&thinsp; `Args`{**his**= history of the RNN as the result of the fit method, **kind**= *\"loss\"*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_curve(estimator, x_train, y_train, x_test, y_test, n_splits, title, zoom_out= False):\n",
    "\n",
    "  split_size = int(len(x_train) / n_splits)\n",
    "  x_splits = np.split(x_train, n_splits) # must be equal division\n",
    "  y_splits = np.split(y_train, n_splits)\n",
    "\n",
    "  train_acc, test_acc, train_prec, test_prec, train_rec, test_rec, train_f, test_f = list(), list(), list(), list(), list(), list(), list(), list()\n",
    "\n",
    "  fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(8, 8), dpi=100, gridspec_kw={'width_ratios': [1, 1], 'height_ratios': [1, 1]})\n",
    "  fig.suptitle(\"Plots for {title}\".format(title=title), fontsize = 16)\n",
    "\n",
    "  curr_x = x_splits[0]\n",
    "  curr_y = y_splits[0]\n",
    "  \n",
    "  estimator.fit(curr_x, curr_y)\n",
    "\n",
    "  train_predict = estimator.predict(curr_x)\n",
    "  test_predict = estimator.predict(x_test)\n",
    "\n",
    "  train_acc.append(accuracy_score(curr_y, train_predict))\n",
    "  test_acc.append(accuracy_score(y_test, test_predict))\n",
    "\n",
    "  train_prec.append(precision_score(curr_y, train_predict))\n",
    "  test_prec.append(precision_score(y_test, test_predict))\n",
    "\n",
    "  train_rec.append(recall_score(curr_y, train_predict))\n",
    "  test_rec.append(recall_score(y_test, test_predict))\n",
    "\n",
    "  train_f.append(f1_score(curr_y, train_predict))\n",
    "  test_f.append(f1_score(y_test, test_predict))\n",
    "\n",
    "  for i in range(1, len(x_splits)):\n",
    "    print(\"Split training examples : \" ,i)\n",
    "    curr_x = np.concatenate((curr_x, x_splits[i]), axis=0)\n",
    "    curr_y = np.concatenate((curr_y, y_splits[i]), axis=0)\n",
    "    estimator.fit(curr_x, curr_y)\n",
    "\n",
    "    train_predict = estimator.predict(curr_x)\n",
    "    test_predict = estimator.predict(x_test)\n",
    "\n",
    "    train_acc.append(accuracy_score(curr_y, train_predict))\n",
    "    test_acc.append(accuracy_score(y_test, test_predict))\n",
    "\n",
    "    train_prec.append(precision_score(curr_y, train_predict))\n",
    "    test_prec.append(precision_score(y_test, test_predict))\n",
    "\n",
    "    train_rec.append(recall_score(curr_y, train_predict))\n",
    "    test_rec.append(recall_score(y_test, test_predict))\n",
    "\n",
    "    train_f.append(f1_score(curr_y, train_predict))\n",
    "    test_f.append(f1_score(y_test, test_predict))\n",
    "    \n",
    "  x = list(range(split_size, len(x_train) + split_size, split_size))\n",
    "\n",
    "  ax1.plot(x, train_acc, 'o-', color=\"b\",  label='Training Accuracy')\n",
    "  ax1.plot(x, test_acc, 'o-', color=\"red\",label='Test Accuracy')\n",
    "  ax1.legend(loc=\"lower right\")\n",
    "  \n",
    "  ax2.plot(x, train_prec, 'o-', color=\"b\",  label='Training Precision')\n",
    "  ax2.plot(x, test_prec, 'o-', color=\"red\",label='Test Precision')\n",
    "  ax2.legend(loc=\"lower right\")\n",
    "\n",
    "  ax3.plot(x, train_rec, 'o-', color=\"b\",  label='Training Recall')\n",
    "  ax3.plot(x, test_rec, 'o-', color=\"red\",label='Test Recall')\n",
    "  ax3.legend(loc=\"lower right\")\n",
    "\n",
    "  ax4.plot(x, train_f, 'o-', color=\"b\",  label='Training f1')\n",
    "  ax4.plot(x, test_f, 'o-', color=\"red\",label='Test f1')\n",
    "  ax4.legend(loc=\"lower right\")\n",
    "\n",
    "  if zoom_out:\n",
    "    ax1.axis(ymin= 0.0, ymax= 1.0)\n",
    "    ax2.axis(ymin= 0.0, ymax= 1.0)\n",
    "    ax3.axis(ymin= 0.0, ymax= 1.0)\n",
    "    ax4.axis(ymin= 0.0, ymax= 1.0)\n",
    "  plt.show()\n",
    "\n",
    "  data = {\n",
    "    'trainA' : train_acc,\n",
    "    'testA' : test_acc,\n",
    "    'trainP' : train_prec,\n",
    "    'testP' : test_prec,\n",
    "    'trainR' : train_rec,\n",
    "    'testR' : test_rec,\n",
    "    'trainF' : train_f,\n",
    "    'testF' : test_f\n",
    "  }\n",
    "\n",
    "  return data\n",
    "\n",
    "def compare_two_classification_algorithms(estimator1, estimator2, x_train, y_train, x_test, y_test, n_splits, title1, title2, zoom_out= False):\n",
    "  split_size = int(len(x_train) / n_splits)\n",
    "  x_splits = np.split(x_train, n_splits) # must be equal division\n",
    "  y_splits = np.split(y_train, n_splits)\n",
    "\n",
    "  train_acc_1, test_acc_1, train_prec_1, test_prec_1, train_rec_1, test_rec_1, train_f_1, test_f_1 = list(), list(), list(), list(), list(), list(), list(), list()\n",
    "\n",
    "  train_acc_2, test_acc_2, train_prec_2, test_prec_2, train_rec_2, test_rec_2, train_f_2, test_f_2 = list(), list(), list(), list(), list(), list(), list(), list()\n",
    "\n",
    "  fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 16), dpi=100, gridspec_kw={'width_ratios': [1, 1], 'height_ratios': [1, 1]})\n",
    "  fig.suptitle(\"Plots Comparing {title1} vs {title2}\".format(title1=title1, title2= title2), fontsize = 16)\n",
    "\n",
    "  curr_x = x_splits[0]\n",
    "  curr_y = y_splits[0]\n",
    "  \n",
    "  estimator1.fit(curr_x, curr_y)\n",
    "  estimator2.fit(curr_x, curr_y)\n",
    "\n",
    "  train_predict_1 = estimator1.predict(curr_x)\n",
    "  test_predict_1 = estimator1.predict(x_test)\n",
    "\n",
    "  train_predict_2 = estimator2.predict(curr_x)\n",
    "  test_predict_2 = estimator2.predict(x_test)\n",
    "\n",
    "  #estimator 1\n",
    "  train_acc_1.append(accuracy_score(curr_y, train_predict_1))\n",
    "  test_acc_1.append(accuracy_score(y_test, test_predict_1))\n",
    "\n",
    "  train_prec_1.append(precision_score(curr_y, train_predict_1))\n",
    "  test_prec_1.append(precision_score(y_test, test_predict_1))\n",
    "\n",
    "  train_rec_1.append(recall_score(curr_y, train_predict_1))\n",
    "  test_rec_1.append(recall_score(y_test, test_predict_1))\n",
    "\n",
    "  train_f_1.append(f1_score(curr_y, train_predict_1))\n",
    "  test_f_1.append(f1_score(y_test, test_predict_1))\n",
    "  #estimator 2\n",
    "  train_acc_2.append(accuracy_score(curr_y, train_predict_2))\n",
    "  test_acc_2.append(accuracy_score(y_test, test_predict_2))\n",
    "\n",
    "  train_prec_2.append(precision_score(curr_y, train_predict_2))\n",
    "  test_prec_2.append(precision_score(y_test, test_predict_2))\n",
    "\n",
    "  train_rec_2.append(recall_score(curr_y, train_predict_2))\n",
    "  test_rec_2.append(recall_score(y_test, test_predict_2))\n",
    "\n",
    "  train_f_2.append(f1_score(curr_y, train_predict_2))\n",
    "  test_f_2.append(f1_score(y_test, test_predict_2))\n",
    "\n",
    "  for i in range(1, len(x_splits)):\n",
    "    print(\"Split training examples : \" ,i)\n",
    "    curr_x = np.concatenate((curr_x, x_splits[i]), axis=0)\n",
    "    curr_y = np.concatenate((curr_y, y_splits[i]), axis=0)\n",
    "    estimator1.fit(curr_x, curr_y)\n",
    "    estimator2.fit(curr_x, curr_y)\n",
    "\n",
    "    train_predict_1 = estimator1.predict(curr_x)\n",
    "    test_predict_1 = estimator1.predict(x_test)\n",
    "\n",
    "    train_predict_2 = estimator2.predict(curr_x)\n",
    "    test_predict_2 = estimator2.predict(x_test)\n",
    "\n",
    "    #estimator 1\n",
    "    train_acc_1.append(accuracy_score(curr_y, train_predict_1))\n",
    "    test_acc_1.append(accuracy_score(y_test, test_predict_1))\n",
    "\n",
    "    train_prec_1.append(precision_score(curr_y, train_predict_1))\n",
    "    test_prec_1.append(precision_score(y_test, test_predict_1))\n",
    "\n",
    "    train_rec_1.append(recall_score(curr_y, train_predict_1))\n",
    "    test_rec_1.append(recall_score(y_test, test_predict_1))\n",
    "\n",
    "    train_f_1.append(f1_score(curr_y, train_predict_1))\n",
    "    test_f_1.append(f1_score(y_test, test_predict_1))\n",
    "    #estimator 2\n",
    "    train_acc_2.append(accuracy_score(curr_y, train_predict_2))\n",
    "    test_acc_2.append(accuracy_score(y_test, test_predict_2))\n",
    "\n",
    "    train_prec_2.append(precision_score(curr_y, train_predict_2))\n",
    "    test_prec_2.append(precision_score(y_test, test_predict_2))\n",
    "\n",
    "    train_rec_2.append(recall_score(curr_y, train_predict_2))\n",
    "    test_rec_2.append(recall_score(y_test, test_predict_2))\n",
    "\n",
    "    train_f_2.append(f1_score(curr_y, train_predict_2))\n",
    "    test_f_2.append(f1_score(y_test, test_predict_2))\n",
    "\n",
    "\n",
    "  x = list(range(split_size, len(x_train) + split_size, split_size))\n",
    "\n",
    "  ax1.plot(x, train_acc_1, 'o-', color=\"cyan\",  label='Training Accuracy for {title1}'.format(title1= title1))\n",
    "  ax1.plot(x, test_acc_1, 'o-', color=\"maroon\",label='Test Accuracy for {title1}'.format(title1= title1))\n",
    "  ax1.plot(x, train_acc_2, 'o-', color=\"gold\",  label='Training Accuracy for {title2}'.format(title2= title2))\n",
    "  ax1.plot(x, test_acc_2, 'o-', color=\"olive\",label='Test Accuracy for {title2}'.format(title2= title2))\n",
    "  ax1.legend(loc=\"lower right\")\n",
    "  \n",
    "  ax2.plot(x, train_prec_1, 'o-', color=\"cyan\",  label='Training Precision for {title1}'.format(title1= title1))\n",
    "  ax2.plot(x, test_prec_1, 'o-', color=\"maroon\",label='Test Precision for {title1}'.format(title1= title1))\n",
    "  ax2.plot(x, train_prec_2, 'o-', color=\"gold\",  label='Training Precision {title2}'.format(title2= title2))\n",
    "  ax2.plot(x, test_prec_2, 'o-', color=\"olive\",label='Test Precision {title2}'.format(title2= title2))\n",
    "  ax2.legend(loc=\"lower right\")\n",
    "\n",
    "  ax3.plot(x, train_rec_1, 'o-', color=\"cyan\",  label='Training Recall for {title1}'.format(title1= title1))\n",
    "  ax3.plot(x, test_rec_1, 'o-', color=\"maroon\",label='Test Recall for {title1}'.format(title1= title1))\n",
    "  ax3.plot(x, train_rec_2, 'o-', color=\"gold\",  label='Training Recall {title2}'.format(title2= title2))\n",
    "  ax3.plot(x, test_rec_2, 'o-', color=\"olive\",label='Test Recall {title2}'.format(title2= title2))\n",
    "  ax3.legend(loc=\"lower right\")\n",
    "\n",
    "  ax4.plot(x, train_f_1, 'o-', color=\"cyan\",  label='Training f1 {title1}'.format(title1= title1))\n",
    "  ax4.plot(x, test_f_1, 'o-', color=\"maroon\",label='Test f1 {title1}'.format(title1= title1))\n",
    "  ax4.plot(x, train_f_2, 'o-', color=\"gold\",  label='Training f1 {title2}'.format(title2= title2))\n",
    "  ax4.plot(x, test_f_2, 'o-', color=\"olive\",label='Test f1 {title2}'.format(title2= title2))\n",
    "  ax4.legend(loc=\"lower right\")\n",
    "\n",
    "  if zoom_out:\n",
    "    ax1.axis(ymin= 0.0, ymax= 1.0)\n",
    "    ax2.axis(ymin= 0.0, ymax= 1.0)\n",
    "    ax3.axis(ymin= 0.0, ymax= 1.0)\n",
    "    ax4.axis(ymin= 0.0, ymax= 1.0)\n",
    "  plt.show()\n",
    "\n",
    "  data1 = {\n",
    "    'trainA' : train_acc_1,\n",
    "    'testA' : test_acc_1,\n",
    "    'trainP' : train_prec_1,\n",
    "    'testP' : test_prec_1,\n",
    "    'trainR' : train_rec_1,\n",
    "    'testR' : test_rec_1,\n",
    "    'trainF' : train_f_1,\n",
    "    'testF' : test_f_1\n",
    "  }\n",
    "\n",
    "  data2 = {\n",
    "    'trainA' : train_acc_2,\n",
    "    'testA' : test_acc_2,\n",
    "    'trainP' : train_prec_2,\n",
    "    'testP' : test_prec_2,\n",
    "    'trainR' : train_rec_2,\n",
    "    'testR' : test_rec_2,\n",
    "    'trainF' : train_f_2,\n",
    "    'testF' : test_f_2\n",
    "  }\n",
    "\n",
    "  return data1, data2\n",
    "\n",
    "def loss_plot(his, kind):\n",
    "  train = his.history[kind]\n",
    "  val = his.history['val_' + kind]\n",
    "  epochs = range(1, len(train)+1)\n",
    "  plt.figure(figsize=(12,9))\n",
    "  plt.plot(epochs, train, 'b', label='Training ' + kind)\n",
    "  plt.plot(epochs, val, 'orange', label='Validation ' + kind)\n",
    "  plt.title('Training and validation ' + kind) \n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel(kind)\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`6.0 Tables in training and test examples`**\n",
    "> #### After collecting the data from the custom curve or the custom_sklearn_curve, <br> it gives us a table with the parameters inside, but in more suitable manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_tables(x_train_b, data, splits):\n",
    "    data = {\n",
    "    #'Id' : [i + 1 for i in range(len(train_accuracies))], \n",
    "    'Number of Examples' : list(range(int(len(x_train_b)/splits), len(x_train_b) + int(len(x_train_b)/splits), int(len(x_train_b)/splits))), \n",
    "    \"Train Accuracy\" : data['trainA'], \n",
    "    \"Test Accuracy\" : data['testA'],\n",
    "    \"Train Precision\" : data['trainP'],\n",
    "    \"Test Precision\" : data['testP'],\n",
    "    \"Train Recall\" : data['trainR'],\n",
    "    \"Test Recall\" : data['testR'],\n",
    "    \"Train F1\" : data['trainF'],\n",
    "    \"Test F1\" : data['testF']}\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Naive_Bayes()\n",
    "data = custom_curve(estimator, np.concatenate((x_train_binary, x_val_binary), axis= 0), np.concatenate((y_train, y_val), axis= 0), x_test=x_test_binary, y_test=y_test, n_splits=5, title= 'Naive Bayes')\n",
    "pandas_tables(x_test_binary, data, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = ID3(labels=set([x for x in range(len(x_train_binary[0]))]), max_depth=10, dc=0)\n",
    "data = custom_curve(estimator, x_train=np.concatenate((x_train_binary, x_val_binary), axis= 0), y_train=np.concatenate((y_train, y_val), axis= 0), x_test=x_test_binary, y_test=y_test, n_splits=5, title= 'ID3', zoom_out= False)\n",
    "pandas_tables(x_test_binary,data, splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Random_Forest(num_trees=3, m=500, max_depth= 5)\n",
    "data = custom_curve(estimator, x_train=np.concatenate((x_train_binary, x_val_binary), axis= 0), y_train=np.concatenate((y_train, y_val), axis= 0), x_test=x_test_binary, y_test=y_test, n_splits=5, title= 'Random Forest')\n",
    "pandas_tables(x_test_binary, data, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = BernoulliNB()\n",
    "estimator = Naive_Bayes()\n",
    "data1, data2 = compare_two_classification_algorithms(estimator1=estimator, estimator2= nb, x_train= np.concatenate((x_train_binary, x_val_binary), axis= 0), y_train= np.concatenate((y_train, y_val), axis= 0), x_test= x_test_binary, y_test= y_test, n_splits= 5, title1= \"Naive Bayes\", title2= \"BarnouliNB\", zoom_out= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_tables(x_train_binary, data1, splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_tables(x_train_binary, data2, splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=10)\n",
    "estimator = ID3(labels=set([x for x in range(len(x_train_binary[0]))]), max_depth=10, dc=0)\n",
    "data1, data2 = compare_two_classification_algorithms(estimator1=estimator, estimator2= dt, x_train= np.concatenate((x_train_binary, x_val_binary), axis= 0), y_train= np.concatenate((y_train, y_val), axis= 0), x_test= x_test_binary, y_test= y_test, n_splits= 5, title1= \"ID3\", title2= \"Decision Tree Classifier\", zoom_out= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_tables(x_train_binary, data1, splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_tables(x_train_binary, data2, splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=3, max_features=500, max_depth=5)\n",
    "estimator = Random_Forest(num_trees=3, m=500, max_depth= 5)\n",
    "data1, data2 = compare_two_classification_algorithms(estimator1=estimator, estimator2= rf, x_train= np.concatenate((x_train_binary, x_val_binary), axis= 0), y_train= np.concatenate((y_train, y_val), axis= 0), x_test= x_test_binary, y_test= y_test, n_splits= 5, title1= \"Random Forest\", title2= \"Random Forest Classifier\", zoom_out= True)\n",
    "pandas_tables(x_train_binary, data, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_tables(x_train_binary, data1, splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_tables(x_train_binary, data2, splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_ = RNN(epochs=5, vocabulary= vocabulary)\n",
    "data = custom_curve(estimator=imdb_, x_train=np.concatenate((x_train, x_val), axis= 0), y_train=np.concatenate((y_train, y_val), axis= 0), x_test=x_test, y_test=y_test, title=\"Rnn BigRu Curves\", zoom_out=False, n_splits=5)\n",
    "pandas_tables(x_train_binary, data, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_ = RNN(epochs=5, vocabulary=vocabulary)\n",
    "imdb_bigru_history = imdb_.get_history(np.concatenate((x_train, x_val), axis= 0), np.concatenate((y_train, y_val), axis= 0))\n",
    "loss_plot(imdb_bigru_history, 'loss')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "sklearn-IMDB.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "7ce9a19ba76a052f1d55c791929efaf6e79f4d5d7527a208fb4ee342865024d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
